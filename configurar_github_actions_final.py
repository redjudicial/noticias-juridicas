#!/usr/bin/env python3
"""
Configuraci√≥n final de GitHub Actions para scraping autom√°tico
"""

import os
import subprocess
from datetime import datetime

def configurar_github_actions():
    """Configurar GitHub Actions para scraping autom√°tico"""
    
    print("üöÄ **CONFIGURACI√ìN FINAL DE GITHUB ACTIONS**")
    print("=" * 60)
    
    # 1. Verificar que el workflow existe
    workflow_file = ".github/workflows/scraping_noticias.yml"
    
    if os.path.exists(workflow_file):
        print("‚úÖ Workflow de GitHub Actions ya existe")
    else:
        print("‚ùå Workflow no encontrado - creando...")
        crear_workflow()
    
    # 2. Verificar configuraci√≥n de secrets
    print("\nüîê **VERIFICACI√ìN DE SECRETS**")
    print("Aseg√∫rate de tener configurados estos secrets en GitHub:")
    print("   ‚Ä¢ SUPABASE_URL")
    print("   ‚Ä¢ SUPABASE_ANON_KEY") 
    print("   ‚Ä¢ OPENAI_API_KEY")
    
    # 3. Crear script de extracci√≥n completa
    print("\nüìù **CREANDO SCRIPT DE EXTRACCI√ìN COMPLETA**")
    crear_script_extraccion_completa()
    
    # 4. Instrucciones finales
    print("\nüìã **INSTRUCCIONES FINALES**")
    print("1. Ve a tu repositorio en GitHub")
    print("2. Ve a Settings ‚Üí Secrets and variables ‚Üí Actions")
    print("3. Agrega los secrets mencionados arriba")
    print("4. Ve a Actions ‚Üí scraping_noticias")
    print("5. Haz clic en 'Run workflow' para probar")
    print("6. El workflow se ejecutar√° autom√°ticamente cada 30 minutos")
    
    print("\nüéØ **SISTEMA LISTO PARA PRODUCCI√ìN**")
    print("‚úÖ Scrapers optimizados")
    print("‚úÖ IA optimizada (GPT-3.5-turbo)")
    print("‚úÖ Base de datos limpia")
    print("‚úÖ Frontend funcional")
    print("‚úÖ GitHub Actions configurado")

def crear_workflow():
    """Crear workflow de GitHub Actions"""
    
    workflow_content = '''name: Scraping Autom√°tico de Noticias Jur√≠dicas

on:
  schedule:
    # Ejecutar cada 30 minutos
    - cron: '*/30 * * * *'
  workflow_dispatch:  # Permitir ejecuci√≥n manual

jobs:
  scrape-news:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout c√≥digo
      uses: actions/checkout@v3
      
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Instalar dependencias
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Configurar variables de entorno
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        echo "Variables de entorno configuradas"
        
    - name: Ejecutar scraping
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python3 backend/main.py
        
    - name: Notificar resultado
      if: always()
      run: |
        echo "Scraping completado - $(date)"
'''
    
    # Crear directorio si no existe
    os.makedirs(".github/workflows", exist_ok=True)
    
    # Escribir archivo
    with open(workflow_file, 'w') as f:
        f.write(workflow_content)
    
    print("‚úÖ Workflow creado exitosamente")

def crear_script_extraccion_completa():
    """Crear script para extracci√≥n completa desde el 21 de julio"""
    
    script_content = '''#!/usr/bin/env python3
"""
Extracci√≥n completa de noticias desde el 21 de julio de 2025
"""

import os
import sys
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Agregar el directorio backend al path
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from main import SistemaNoticiasJuridicas

def extraccion_completa():
    """Ejecutar extracci√≥n completa desde el 21 de julio"""
    
    print("üöÄ **EXTRACCI√ìN COMPLETA DE NOTICIAS**")
    print("=" * 50)
    print(f"üìÖ Desde: 21 de julio de 2025")
    print(f"üìÖ Hasta: {datetime.now().strftime('%d de %B de %Y')}")
    print("=" * 50)
    
    # Cargar variables de entorno
    load_dotenv('APIS_Y_CREDENCIALES.env')
    
    # Inicializar sistema
    sistema = SistemaNoticiasJuridicas()
    
    # Configurar para extracci√≥n completa
    sistema.max_noticias_por_fuente = 100  # M√°s noticias por fuente
    sistema.fecha_inicio = datetime(2025, 7, 21)  # Desde el 21 de julio
    
    # Ejecutar scraping completo
    try:
        sistema.ejecutar_scraping_completo()
        print("\n‚úÖ **EXTRACCI√ìN COMPLETADA EXITOSAMENTE**")
        print("üéØ Sistema listo para producci√≥n con GitHub Actions")
        
    except Exception as e:
        print(f"\n‚ùå Error en extracci√≥n: {e}")
        sys.exit(1)

if __name__ == "__main__":
    extraccion_completa()
'''
    
    with open('extraccion_completa_final.py', 'w') as f:
        f.write(script_content)
    
    print("‚úÖ Script de extracci√≥n completa creado")

if __name__ == "__main__":
    configurar_github_actions() 