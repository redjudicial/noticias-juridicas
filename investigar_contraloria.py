#!/usr/bin/env python3
"""
Script para investigar la estructura real de la p√°gina de Contralor√≠a
"""

import requests
from bs4 import BeautifulSoup
import re

def investigar_contraloria():
    """Investigar la estructura de la p√°gina de Contralor√≠a"""
    url = "https://www.contraloria.cl/portalweb/web/cgr/noticias"
    
    try:
        print("üîç INVESTIGANDO ESTRUCTURA DE CONTRALOR√çA")
        print("=" * 60)
        
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print(f"üìÑ T√≠tulo de la p√°gina: {soup.title.string if soup.title else 'Sin t√≠tulo'}")
        print(f"üìä Tama√±o del contenido: {len(response.content)} bytes")
        print()
        
        # Buscar todos los enlaces
        print("üîó ENLACES ENCONTRADOS:")
        print("-" * 40)
        
        links = soup.find_all('a', href=True)
        noticia_links = []
        
        for link in links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # Filtrar enlaces de noticias
            if (href and 
                text and 
                len(text) > 10 and
                ('noticias' in href.lower() or 'content' in href.lower()) and
                not any(excl in text.lower() for excl in ['inicio', 'menu', 'contacto', 'transparencia', 'p√°gina', 'anterior', 'siguiente'])):
                
                noticia_links.append({
                    'href': href,
                    'text': text,
                    'full_url': href if href.startswith('http') else f"https://www.contraloria.cl{href}"
                })
        
        print(f"üì∞ Enlaces de noticias encontrados: {len(noticia_links)}")
        for i, link in enumerate(noticia_links[:5], 1):
            print(f"   {i}. {link['text'][:80]}...")
            print(f"      URL: {link['full_url']}")
            print()
        
        # Probar extraer contenido de una noticia
        if noticia_links:
            test_url = noticia_links[0]['full_url']
            print(f"üß™ PROBANDO EXTRACCI√ìN DE CONTENIDO:")
            print(f"URL de prueba: {test_url}")
            print("-" * 40)
            
            try:
                test_response = requests.get(test_url, timeout=30)
                test_response.raise_for_status()
                
                test_soup = BeautifulSoup(test_response.content, 'html.parser')
                
                print(f"üìÑ T√≠tulo de la noticia: {test_soup.title.string if test_soup.title else 'Sin t√≠tulo'}")
                print(f"üìä Tama√±o del contenido: {len(test_response.content)} bytes")
                
                # Buscar contenido
                content_selectors = [
                    '.entry-content',
                    '.post-content',
                    '.noticia-contenido',
                    '.content',
                    'article',
                    '.main-content',
                    '.noticia-body',
                    '.journal-content-article',
                    '.portlet-body',
                    '.asset-content'
                ]
                
                print("\nüîç BUSCANDO CONTENIDO:")
                for selector in content_selectors:
                    elem = test_soup.select_one(selector)
                    if elem:
                        text = elem.get_text(strip=True)
                        print(f"‚úÖ Selector '{selector}': {len(text)} caracteres")
                        print(f"   Primeros 200 caracteres: {text[:200]}...")
                        break
                else:
                    print("‚ùå No se encontr√≥ contenido con los selectores est√°ndar")
                    
                    # Buscar cualquier div con contenido
                    divs = test_soup.find_all('div')
                    for div in divs:
                        text = div.get_text(strip=True)
                        if len(text) > 500:  # Contenido sustancial
                            classes = ' '.join(div.get('class', []))
                            print(f"üìù Div con clase '{classes}': {len(text)} caracteres")
                            print(f"   Primeros 200 caracteres: {text[:200]}...")
                            break
                
            except Exception as e:
                print(f"‚ùå Error accediendo a la noticia: {e}")
        
    except Exception as e:
        print(f"‚ùå Error investigando Contralor√≠a: {e}")

if __name__ == "__main__":
    investigar_contraloria() 