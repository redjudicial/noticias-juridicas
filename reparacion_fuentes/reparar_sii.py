#!/usr/bin/env python3
"""
Reparaci√≥n espec√≠fica del problema de SII que no se actualiza desde 31 julio
"""

import os
import sys
import requests
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv('../APIS_Y_CREDENCIALES.env')

# Configuraci√≥n de Supabase
SUPABASE_URL = os.getenv('SUPABASE_URL', 'https://qfomiierchksyfhxoukj.supabase.co')
SUPABASE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')

def verificar_urls_sii():
    """Verificar URLs del SII y su estructura actual"""
    print("üîó **VERIFICACI√ìN DE URLs - SII**")
    print("=" * 50)
    
    urls_a_verificar = [
        "https://www.sii.cl/",
        "https://www.sii.cl/noticias/",
        "https://www.sii.cl/noticias/2025/",
        "https://www.sii.cl/noticias/2025/index.html"
    ]
    
    resultados = {}
    
    for url in urls_a_verificar:
        try:
            response = requests.get(url, timeout=10)
            resultados[url] = {
                'status': response.status_code,
                'accessible': response.status_code == 200,
                'content_length': len(response.text) if response.status_code == 200 else 0
            }
            
            if response.status_code == 200:
                print(f"‚úÖ {url} - Accesible ({len(response.text)} caracteres)")
            else:
                print(f"‚ùå {url} - Error {response.status_code}")
                
        except Exception as e:
            resultados[url] = {
                'status': 'error',
                'accessible': False,
                'error': str(e)
            }
            print(f"‚ùå {url} - Error de conexi√≥n: {e}")
    
    return resultados

def analizar_estructura_sii():
    """Analizar la estructura actual de la p√°gina de noticias del SII"""
    print("\nüîç **AN√ÅLISIS DE ESTRUCTURA - SII**")
    print("=" * 50)
    
    try:
        # Acceder a la p√°gina principal de noticias
        url = "https://www.sii.cl/noticias/"
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            html = response.text
            print("‚úÖ P√°gina de noticias accesible")
            
            # Buscar patrones espec√≠ficos
            patrones = {
                'enlaces_htm': '.htm' in html,
                'referencias_2025': '2025' in html,
                'estructura_noticias': 'noticias' in html.lower(),
                'tabla_noticias': 'table' in html.lower(),
                'lista_noticias': 'ul' in html.lower() or 'ol' in html.lower(),
                'enlaces_noticias': 'href=' in html and 'noticias' in html
            }
            
            print("\nüìä **PATRONES ENCONTRADOS:**")
            for patron, encontrado in patrones.items():
                status = "‚úÖ" if encontrado else "‚ùå"
                print(f"   {status} {patron}")
            
            # Buscar enlaces espec√≠ficos
            if '.htm' in html:
                print("\nüîó **ENLACES .HTM ENCONTRADOS:**")
                lineas = html.split('\n')
                for i, linea in enumerate(lineas):
                    if '.htm' in linea and 'href' in linea:
                        print(f"   L√≠nea {i+1}: {linea.strip()[:100]}...")
                        if i > 10:  # Limitar a los primeros 10
                            break
            
            return patrones
            
        else:
            print(f"‚ùå Error accediendo a la p√°gina: {response.status_code}")
            return None
            
    except Exception as e:
        print(f"‚ùå Error analizando estructura: {e}")
        return None

def verificar_scraper_sii():
    """Verificar el scraper de SII actual"""
    print("\nüîß **VERIFICACI√ìN DEL SCRAPER - SII**")
    print("=" * 50)
    
    scraper_path = "../backend/scrapers/fuentes/sii/sii_scraper.py"
    
    if not os.path.exists(scraper_path):
        print("‚ùå Archivo del scraper no encontrado")
        return None
    
    try:
        with open(scraper_path, 'r') as f:
            contenido = f.read()
        
        print("‚úÖ Archivo del scraper encontrado")
        
        # Analizar URLs en el scraper
        urls_en_scraper = []
        lineas = contenido.split('\n')
        for i, linea in enumerate(lineas):
            if 'sii.cl' in linea:
                urls_en_scraper.append((i+1, linea.strip()))
        
        print(f"\nüîó **URLs EN EL SCRAPER:**")
        for num_linea, linea in urls_en_scraper:
            print(f"   L√≠nea {num_linea}: {linea}")
        
        # Analizar selectores
        selectores = []
        for i, linea in enumerate(lineas):
            if any(selector in linea.lower() for selector in ['selector', 'xpath', 'css', 'find']):
                selectores.append((i+1, linea.strip()))
        
        print(f"\nüéØ **SELECTORES ENCONTRADOS:**")
        for num_linea, linea in selectores[:10]:  # Mostrar solo los primeros 10
            print(f"   L√≠nea {num_linea}: {linea}")
        
        return {
            'urls': urls_en_scraper,
            'selectores': selectores,
            'contenido': contenido
        }
        
    except Exception as e:
        print(f"‚ùå Error leyendo archivo: {e}")
        return None

def probar_scraper_sii_manual():
    """Probar el scraper de SII manualmente"""
    print("\nüß™ **PRUEBA MANUAL DEL SCRAPER - SII**")
    print("=" * 50)
    
    try:
        # Intentar acceder directamente a la URL de noticias del SII
        url_noticias = "https://www.sii.cl/noticias/2025/"
        response = requests.get(url_noticias, timeout=10)
        
        if response.status_code == 200:
            html = response.text
            print("‚úÖ URL de noticias accesible")
            
            # Buscar enlaces a noticias espec√≠ficas
            if '.htm' in html:
                print("‚úÖ Se encontraron enlaces .htm")
                
                # Extraer enlaces manualmente
                enlaces = []
                lineas = html.split('\n')
                for linea in lineas:
                    if '.htm' in linea and 'href' in linea:
                        # Extraer URL del enlace
                        if 'href="' in linea:
                            inicio = linea.find('href="') + 6
                            fin = linea.find('"', inicio)
                            if fin > inicio:
                                url_relativa = linea[inicio:fin]
                                if url_relativa.startswith('./'):
                                    url_relativa = url_relativa[2:]
                                url_completa = f"https://www.sii.cl/noticias/2025/{url_relativa}"
                                enlaces.append(url_completa)
                
                print(f"\nüîó **ENLACES ENCONTRADOS ({len(enlaces)}):**")
                for i, enlace in enumerate(enlaces[:5], 1):  # Mostrar solo los primeros 5
                    print(f"   {i}. {enlace}")
                
                # Probar acceder a una noticia espec√≠fica
                if enlaces:
                    print(f"\nüîç **PROBANDO ACCESO A NOTICIA:**")
                    url_prueba = enlaces[0]
                    try:
                        response_noticia = requests.get(url_prueba, timeout=10)
                        if response_noticia.status_code == 200:
                            print(f"‚úÖ {url_prueba} - Accesible")
                            
                            # Buscar t√≠tulo en la noticia
                            html_noticia = response_noticia.text
                            if '<title>' in html_noticia:
                                inicio = html_noticia.find('<title>') + 7
                                fin = html_noticia.find('</title>', inicio)
                                if fin > inicio:
                                    titulo = html_noticia[inicio:fin].strip()
                                    print(f"   T√≠tulo: {titulo}")
                        else:
                            print(f"‚ùå {url_prueba} - Error {response_noticia.status_code}")
                    except Exception as e:
                        print(f"‚ùå {url_prueba} - Error de conexi√≥n: {e}")
                
                return enlaces
            else:
                print("‚ùå No se encontraron enlaces .htm")
                return []
        else:
            print(f"‚ùå Error accediendo a URL de noticias: {response.status_code}")
            return []
            
    except Exception as e:
        print(f"‚ùå Error en prueba manual: {e}")
        return []

def generar_solucion_sii():
    """Generar soluci√≥n para el problema de SII"""
    print("\nüí° **SOLUCI√ìN PARA SII**")
    print("=" * 50)
    
    print("üîß **PROBLEMAS IDENTIFICADOS:**")
    print("   - √öltima noticia del 31 de julio")
    print("   - Posible cambio en estructura de p√°gina")
    print("   - URLs o selectores desactualizados")
    
    print("\nüîß **SOLUCIONES PROPUESTAS:**")
    print("1. **Verificar estructura actual:**")
    print("   - Analizar HTML de la p√°gina de noticias")
    print("   - Identificar nuevos patrones de enlaces")
    print("   - Actualizar selectores si es necesario")
    
    print("\n2. **Actualizar URLs:**")
    print("   - Verificar que las URLs siguen siendo v√°lidas")
    print("   - Buscar nuevas URLs de noticias")
    print("   - Implementar redirecciones si es necesario")
    
    print("\n3. **Mejorar extracci√≥n:**")
    print("   - Implementar extracci√≥n m√°s robusta")
    print("   - Agregar manejo de errores")
    print("   - Implementar reintentos")
    
    print("\n4. **Verificar contenido:**")
    print("   - Confirmar que el SII ha publicado noticias nuevas")
    print("   - Verificar fechas de publicaci√≥n")
    print("   - Implementar filtros de fecha")

def crear_scraper_sii_mejorado():
    """Crear una versi√≥n mejorada del scraper de SII"""
    print("\nüîß **CREANDO SCRAPER MEJORADO - SII**")
    print("=" * 50)
    
    scraper_mejorado = '''
#!/usr/bin/env python3
"""
Scraper mejorado para SII
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re

class SIIScraperMejorado:
    def __init__(self):
        self.base_url = "https://www.sii.cl"
        self.noticias_url = "https://www.sii.cl/noticias/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def obtener_noticias(self):
        """Obtener noticias del SII con manejo mejorado"""
        try:
            # Intentar diferentes URLs
            urls_a_probar = [
                "https://www.sii.cl/noticias/",
                "https://www.sii.cl/noticias/2025/",
                "https://www.sii.cl/noticias/2025/index.html"
            ]
            
            for url in urls_a_probar:
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        print(f"‚úÖ URL accesible: {url}")
                        return self.extraer_noticias(response.text, url)
                except Exception as e:
                    print(f"‚ùå Error con URL {url}: {e}")
                    continue
            
            print("‚ùå No se pudo acceder a ninguna URL del SII")
            return []
            
        except Exception as e:
            print(f"‚ùå Error general: {e}")
            return []
    
    def extraer_noticias(self, html, url_base):
        """Extraer noticias del HTML"""
        noticias = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # Buscar enlaces a noticias
        enlaces = soup.find_all('a', href=True)
        
        for enlace in enlaces:
            href = enlace.get('href')
            if '.htm' in href:
                # Construir URL completa
                if href.startswith('./'):
                    href = href[2:]
                elif not href.startswith('http'):
                    href = f"{url_base.rstrip('/')}/{href}"
                
                # Extraer informaci√≥n de la noticia
                titulo = enlace.get_text(strip=True)
                if titulo:
                    noticia = {
                        'titulo': titulo,
                        'url': href,
                        'fuente': 'sii',
                        'fecha_publicacion': datetime.now().isoformat()
                    }
                    noticias.append(noticia)
        
        return noticias
    
    def scrape(self):
        """M√©todo principal de scraping"""
        print("üîç Iniciando scraping del SII...")
        return self.obtener_noticias()

# Uso del scraper
if __name__ == "__main__":
    scraper = SIIScraperMejorado()
    noticias = scraper.scrape()
    print(f"üìä Noticias encontradas: {len(noticias)}")
    for noticia in noticias[:3]:
        print(f"   - {noticia['titulo']}")
'''
    
    # Guardar el scraper mejorado
    with open('sii_scraper_mejorado.py', 'w') as f:
        f.write(scraper_mejorado)
    
    print("‚úÖ Scraper mejorado creado: sii_scraper_mejorado.py")
    print("\nüìã **CARACTER√çSTICAS DEL SCRAPER MEJORADO:**")
    print("   - M√∫ltiples URLs de respaldo")
    print("   - Manejo de errores robusto")
    print("   - Extracci√≥n flexible de enlaces")
    print("   - Headers de navegador realistas")
    print("   - Timeout configurado")

def main():
    """Funci√≥n principal"""
    print("üîß **REPARACI√ìN ESPEC√çFICA - SII**")
    print("=" * 70)
    print(f"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # Ejecutar an√°lisis y reparaci√≥n
    verificar_urls_sii()
    analizar_estructura_sii()
    verificar_scraper_sii()
    probar_scraper_sii_manual()
    generar_solucion_sii()
    crear_scraper_sii_mejorado()
    
    print(f"\n‚úÖ **REPARACI√ìN COMPLETADA**")
    print("=" * 70)

if __name__ == "__main__":
    main() 