#!/usr/bin/env python3
"""
AnÃ¡lisis especÃ­fico del problema de hash duplicado en ContralorÃ­a
"""

import os
import sys
import requests
import hashlib
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv('../APIS_Y_CREDENCIALES.env')

# ConfiguraciÃ³n de Supabase
SUPABASE_URL = os.getenv('SUPABASE_URL', 'https://qfomiierchksyfhxoukj.supabase.co')
SUPABASE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')

def obtener_noticias_contraloria():
    """Obtener noticias de ContralorÃ­a de la base de datos"""
    try:
        headers = {
            'apikey': SUPABASE_KEY,
            'Authorization': f'Bearer {SUPABASE_KEY}'
        }
        
        response = requests.get(
            f'{SUPABASE_URL}/rest/v1/noticias_juridicas?select=*&fuente=eq.contraloria&order=fecha_publicacion.desc',
            headers=headers
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âŒ Error al obtener noticias: {response.status_code}")
            return []
            
    except Exception as e:
        print(f"âŒ Error en la peticiÃ³n: {e}")
        return []

def analizar_hashes_duplicados():
    """Analizar hashes duplicados en noticias de ContralorÃ­a"""
    print("ğŸ” **ANÃLISIS DE HASHES DUPLICADOS - CONTRALORÃA**")
    print("=" * 60)
    
    noticias = obtener_noticias_contraloria()
    
    if not noticias:
        print("âŒ No se pudieron obtener noticias de ContralorÃ­a")
        return
    
    print(f"ğŸ“Š Total noticias de ContralorÃ­a: {len(noticias)}")
    
    # Analizar hashes
    hashes = {}
    duplicados = []
    
    for noticia in noticias:
        hash_contenido = noticia.get('hash_contenido')
        titulo = noticia.get('titulo', 'Sin tÃ­tulo')
        fecha = noticia.get('fecha_publicacion', 'Sin fecha')
        url = noticia.get('url_origen', 'Sin URL')
        
        if hash_contenido:
            if hash_contenido in hashes:
                duplicados.append({
                    'hash': hash_contenido,
                    'titulo1': hashes[hash_contenido]['titulo'],
                    'fecha1': hashes[hash_contenido]['fecha'],
                    'url1': hashes[hash_contenido]['url'],
                    'titulo2': titulo,
                    'fecha2': fecha,
                    'url2': url
                })
            else:
                hashes[hash_contenido] = {
                    'titulo': titulo,
                    'fecha': fecha,
                    'url': url
                }
    
    print(f"\nğŸ“Š **ANÃLISIS DE DUPLICADOS:**")
    print("-" * 40)
    print(f"Total noticias: {len(noticias)}")
    print(f"Hashes Ãºnicos: {len(hashes)}")
    print(f"Duplicados encontrados: {len(duplicados)}")
    
    if duplicados:
        print(f"\nğŸš¨ **DUPLICADOS DETECTADOS:**")
        print("-" * 40)
        for i, dup in enumerate(duplicados[:10], 1):  # Mostrar solo los primeros 10
            print(f"{i}. Hash: {dup['hash'][:20]}...")
            print(f"   Noticia 1: {dup['titulo1'][:50]}... ({dup['fecha1'][:10]})")
            print(f"   Noticia 2: {dup['titulo2'][:50]}... ({dup['fecha2'][:10]})")
            print(f"   URLs: {dup['url1'] != dup['url2']}")
            print()
    
    return duplicados

def analizar_fechas_contraloria():
    """Analizar fechas de las noticias de ContralorÃ­a"""
    print("\nğŸ“… **ANÃLISIS DE FECHAS - CONTRALORÃA**")
    print("-" * 40)
    
    noticias = obtener_noticias_contraloria()
    
    if not noticias:
        return
    
    # Agrupar por fecha
    fechas = {}
    for noticia in noticias:
        fecha = noticia.get('fecha_publicacion', 'Sin fecha')
        if fecha != 'Sin fecha':
            fecha_simple = fecha[:10]  # Solo la fecha
            if fecha_simple not in fechas:
                fechas[fecha_simple] = 0
            fechas[fecha_simple] += 1
    
    print("ğŸ“Š Noticias por fecha:")
    for fecha in sorted(fechas.keys(), reverse=True):
        print(f"  {fecha}: {fechas[fecha]} noticias")
    
    # Verificar la Ãºltima noticia
    if noticias:
        ultima = noticias[0]
        fecha_ultima = datetime.fromisoformat(ultima['fecha_publicacion'].replace('Z', '+00:00'))
        ahora = datetime.now(fecha_ultima.tzinfo)
        diferencia = ahora - fecha_ultima
        
        print(f"\nâ° Ãšltima noticia: {fecha_ultima.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° Tiempo transcurrido: {diferencia}")
        
        if diferencia.days > 1:
            print("âš ï¸ Â¡ALERTA! No hay noticias nuevas en mÃ¡s de 1 dÃ­a")

def verificar_urls_contraloria():
    """Verificar que las URLs de ContralorÃ­a siguen siendo vÃ¡lidas"""
    print("\nğŸ”— **VERIFICACIÃ“N DE URLs - CONTRALORÃA**")
    print("-" * 40)
    
    noticias = obtener_noticias_contraloria()
    
    if not noticias:
        return
    
    # Tomar las Ãºltimas 5 noticias para verificar
    noticias_recientes = noticias[:5]
    
    print("ğŸ” Verificando URLs de las Ãºltimas 5 noticias:")
    
    for i, noticia in enumerate(noticias_recientes, 1):
        url = noticia.get('url_origen', '')
        titulo = noticia.get('titulo', 'Sin tÃ­tulo')
        
        if url:
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    print(f"âœ… {i}. {titulo[:40]}... - URL vÃ¡lida")
                else:
                    print(f"âŒ {i}. {titulo[:40]}... - Error {response.status_code}")
            except Exception as e:
                print(f"âŒ {i}. {titulo[:40]}... - Error de conexiÃ³n: {e}")
        else:
            print(f"âš ï¸ {i}. {titulo[:40]}... - Sin URL")

def analizar_estructura_hash():
    """Analizar cÃ³mo se genera el hash en ContralorÃ­a"""
    print("\nğŸ”§ **ANÃLISIS DE GENERACIÃ“N DE HASH**")
    print("-" * 40)
    
    # Buscar el scraper de ContralorÃ­a
    scraper_path = "../backend/scrapers/fuentes/contraloria/contraloria_scraper.py"
    
    if os.path.exists(scraper_path):
        print("âœ… Archivo del scraper encontrado")
        
        try:
            with open(scraper_path, 'r') as f:
                contenido = f.read()
            
            # Buscar funciones relacionadas con hash
            if 'hash' in contenido.lower():
                print("âœ… CÃ³digo de hash encontrado en el scraper")
                
                # Buscar lÃ­neas especÃ­ficas
                lineas = contenido.split('\n')
                for i, linea in enumerate(lineas):
                    if 'hash' in linea.lower():
                        print(f"   LÃ­nea {i+1}: {linea.strip()}")
            else:
                print("âš ï¸ No se encontrÃ³ cÃ³digo de hash en el scraper")
                
        except Exception as e:
            print(f"âŒ Error leyendo archivo: {e}")
    else:
        print("âŒ Archivo del scraper no encontrado")

def generar_recomendaciones():
    """Generar recomendaciones especÃ­ficas para ContralorÃ­a"""
    print("\nğŸ’¡ **RECOMENDACIONES PARA CONTRALORÃA**")
    print("-" * 40)
    
    print("1. ğŸ”§ **PROBLEMA DE HASH DUPLICADO:**")
    print("   - Revisar lÃ³gica de generaciÃ³n de hash")
    print("   - Verificar que el hash incluya elementos Ãºnicos")
    print("   - Implementar verificaciÃ³n de duplicados antes de insertar")
    
    print("\n2. ğŸ” **VERIFICACIÃ“N DE DUPLICADOS:**")
    print("   - Buscar noticias existentes por URL antes de insertar")
    print("   - Implementar verificaciÃ³n por tÃ­tulo y fecha")
    print("   - Usar hash como respaldo, no como Ãºnico identificador")
    
    print("\n3. ğŸ“… **ACTUALIZACIÃ“N DE FECHAS:**")
    print("   - Verificar que las fechas se extraen correctamente")
    print("   - Implementar filtro por fecha para evitar noticias antiguas")
    print("   - Agregar logging para monitorear fechas extraÃ­das")
    
    print("\n4. ğŸ”— **VERIFICACIÃ“N DE URLs:**")
    print("   - Verificar que las URLs siguen siendo vÃ¡lidas")
    print("   - Implementar manejo de errores para URLs rotas")
    print("   - Agregar timeout y reintentos para conexiones")

def main():
    """FunciÃ³n principal"""
    print("ğŸ” **ANÃLISIS ESPECÃFICO - CONTRALORÃA**")
    print("=" * 70)
    print(f"ğŸ“… Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # Ejecutar anÃ¡lisis
    duplicados = analizar_hashes_duplicados()
    analizar_fechas_contraloria()
    verificar_urls_contraloria()
    analizar_estructura_hash()
    generar_recomendaciones()
    
    print(f"\nâœ… **ANÃLISIS COMPLETADO**")
    print("=" * 70)
    
    if duplicados:
        print(f"ğŸš¨ Se encontraron {len(duplicados)} duplicados que necesitan atenciÃ³n")
    else:
        print("âœ… No se encontraron duplicados")

if __name__ == "__main__":
    main() 