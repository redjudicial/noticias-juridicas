name: Scraping AutomÃ¡tico de Noticias JurÃ­dicas (24/7 - Cada Hora)

on:
  schedule:
    # Ejecutar cada hora, 24/7
    # Cron: minuto hora dÃ­a mes dÃ­a_semana
    # 0 = cada hora en el minuto 0
    # * = todos los dÃ­as del mes
    # * = todos los meses
    # * = todos los dÃ­as de la semana
    - cron: '0 * * * *'
  
  workflow_dispatch:  # Permitir ejecuciÃ³n manual
    inputs:
      test_mode:
        description: 'Ejecutar en modo prueba (solo 2 fuentes)'
        required: false
        default: 'false'
        type: boolean
      full_run:
        description: 'Ejecutar todas las fuentes (incluye las que no funcionan)'
        required: false
        default: 'false'
        type: boolean

jobs:
  scraping-noticias:
    runs-on: ubuntu-latest
    timeout-minutes: 25  # Aumentado para evitar cortes en ejecuciones mÃ¡s lentas
    
    steps:
    - name: Checkout cÃ³digo
      uses: actions/checkout@v4
      
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
        
    - name: Instalar dependencias
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Configurar variables de entorno
      run: |
        echo "Configurando variables de entorno..."
        echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" >> $GITHUB_ENV
        echo "SUPABASE_ANON_KEY=${{ secrets.SUPABASE_ANON_KEY }}" >> $GITHUB_ENV
        echo "SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" >> $GITHUB_ENV
        echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> $GITHUB_ENV

    - name: Verificar configuraciÃ³n de Supabase (previo)
      run: |
        echo "ğŸ” Verificando variables de entorno de Supabase..."
        echo "SUPABASE_URL=$SUPABASE_URL"
        echo "Probando lectura de conteo de noticias (previo)..."
        curl -s -H "apikey: $SUPABASE_ANON_KEY" -H "Authorization: Bearer $SUPABASE_ANON_KEY" \
          "$SUPABASE_URL/rest/v1/noticias_juridicas?select=count" | cat
        
    - name: Crear archivo de configuraciÃ³n
      run: |
        cat > APIS_Y_CREDENCIALES.env << EOF
        SUPABASE_URL=${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY=${{ secrets.SUPABASE_ANON_KEY }}
        SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}
        EOF
        
    - name: Ejecutar scraping optimizado
      run: |
        echo "ğŸš€ Iniciando scraping automÃ¡tico 24/7 (cada hora)..."
        echo "ğŸ“… Fecha: $(date)"
        echo "â° Hora: $(date +%H:%M:%S)"
        
        if [ "${{ github.event.inputs.test_mode }}" = "true" ]; then
          echo "ğŸ§ª MODO PRUEBA: Solo fuentes funcionando"
          python3 backend/main.py --once --test-mode --max-noticias 5
        elif [ "${{ github.event.inputs.full_run }}" = "true" ]; then
          echo "ğŸ­ MODO COMPLETO: Todas las fuentes"
          python3 backend/main.py --once --max-noticias 10
        else
          echo "âš¡ MODO OPTIMIZADO: Solo fuentes funcionando, pocas noticias"
          python3 backend/main.py --once --working-only --max-noticias 3
        fi

    - name: DiagnÃ³stico Supabase (post-scraping)
      run: |
        echo "ğŸ“Š Verificando conteo de noticias (posterior al scraping)..."
        curl -s -H "apikey: $SUPABASE_ANON_KEY" -H "Authorization: Bearer $SUPABASE_ANON_KEY" \
          "$SUPABASE_URL/rest/v1/noticias_juridicas?select=count" | cat

    - name: Test de conexiÃ³n a Supabase (detallado)
      if: always()
      run: |
        echo "ğŸ§ª Ejecutando test detallado de conexiÃ³n a Supabase..."
        python3 test_supabase_connection.py || true
        
    - name: Verificar resultados
      run: |
        echo "ğŸ“Š Verificando resultados del scraping..."
        # VerificaciÃ³n rÃ¡pida
        echo "âœ… Scraping completado exitosamente"
        
    - name: Limpiar archivos temporales
      if: always()
      run: |
        echo "ğŸ§¹ Limpiando archivos temporales..."
        rm -f APIS_Y_CREDENCIALES.env
        rm -f *.log
        rm -f scraping.pid
        
  # Job de monitoreo simplificado (solo si es necesario)
  monitoreo-rapido:
    runs-on: ubuntu-latest
    needs: scraping-noticias
    if: github.event.inputs.full_run == 'true' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout cÃ³digo
      uses: actions/checkout@v4
      
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Instalar dependencias
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Configurar variables de entorno
      run: |
        echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" >> $GITHUB_ENV
        echo "SUPABASE_ANON_KEY=${{ secrets.SUPABASE_ANON_KEY }}" >> $GITHUB_ENV
        echo "SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" >> $GITHUB_ENV
        echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> $GITHUB_ENV
        
    - name: Generar estadÃ­sticas rÃ¡pidas
      run: |
        echo "ğŸ“ˆ Generando estadÃ­sticas rÃ¡pidas..."
        python3 backend/main.py --stats --quick 